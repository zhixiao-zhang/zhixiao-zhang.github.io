title: Memory Safety / PL / Systems Radar
description: Recent arXiv papers filtered by my interests in systems security, PL,
  and memory safety.
generated_at: '2026-01-14T13:43:48.228281+00:00'
papers:
- arxiv_id: 2601.08770v1
  title: 'Memory DisOrder: Memory Re-orderings as a Timerless Side-channel'
  authors:
  - Sean Siddens
  - Sanya Srivastava
  - Reese Levine
  - Josiah Dykstra
  - Tyler Sorensen
  summary: "To improve efficiency, nearly all parallel processing units (CPUs and\
    \ GPUs) implement relaxed memory models in which memory operations may be re-ordered,\
    \ i.e., executed out-of-order. Prior testing work in this area found that memory\
    \ re-orderings are observed more frequently when other cores are active, e.g.,\
    \ stressing the memory system, which likely triggers aggressive hardware optimizations.\n\
    \  In this work, we present Memory DisOrder: a timerless side-channel that uses\
    \ memory re-orderings to infer activity on other processes. We first perform a\
    \ fuzzing campaign and show that many mainstream processors (X86/Arm/Apple CPUs,\
    \ NVIDIA/AMD/Apple GPUs) are susceptible to cross-process signals. We then show\
    \ how the vulnerability can be used to implement classic attacks, including a\
    \ covert channel, achieving up to 16 bits/second with 95% accuracy on an Apple\
    \ M3 GPU, and application fingerprinting, achieving reliable closed-world DNN\
    \ architecture fingerprinting on several CPUs and an Apple M3 GPU. Finally, we\
    \ explore how low-level system details can be exploited to increase re-orderings,\
    \ showing the potential for a covert channel to achieve nearly 30K bits/second\
    \ on X86 CPUs. More precise attacks can likely be developed as the vulnerability\
    \ becomes better understood."
  tldr: '- **Problem Addressed:** Relaxed memory models in CPUs/GPUs allow memory
    operations to be re-ordered, and this re-ordering frequency can be influenced
    by activity from other processes, creating a side-channel that does not rely on
    traditional timers.


    - **Core Technical Method:** A fuzzing campaign identifies that memory re-orderings
    can be observed across processes on mainstream processors (X86, Arm, Apple CPUs,
    and NVIDIA/AMD/Apple GPUs). These re-orderings are used to infer other processes''
    activity, enabling timerless side-channel attacks such as covert communication
    and application fingerprinting.


    - **Impact on Security:** This exposes a new side-channel vector that bypasses
    timer-based mitigations, allowing cross-process information leakage. Demonstrated
    attacks include covert channels (up to 30 Kb/s on X86 CPUs) and reliable fingerprinting
    of applications like DNN architectures, posing risks to memory safety and system
    isolation in shared hardware environments.'
  ai_score: 8
  ai_reason: The paper directly addresses memory safety and program analysis by investigating
    memory re-orderings as a side-channel, which relates to formal semantics, concurrency,
    and low-level system behavior relevant to memory and spatial safety in systems
    like Rust.
  updated: '2026-01-13T17:59:28Z'
  published: '2026-01-13T17:59:28Z'
  pdf_url: https://arxiv.org/pdf/2601.08770v1
  primary_category: cs.CR
  categories:
  - cs.CR
  - cs.AR
- arxiv_id: 2601.05467v2
  title: 'STELP: Secure Transpilation and Execution of LLM-Generated Programs'
  authors:
  - Swapnil Shinde
  - Sahil Wadhwa
  - Andy Luo
  - Akshay Gupta
  - Mohammad Shahed Sorower
  summary: 'Rapid evolution of Large Language Models (LLMs) has achieved major advances
    in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative
    frameworks using such LLMs place them at the center of solving software development-related
    tasks such as code generation. However, direct use of LLM generated code in production
    software development systems is problematic. The code could be unstable or erroneous
    and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations
    that could lead to widespread system malfunctions. This prohibits the adoption
    of LLM generated code in production AI systems where human code reviews and traditional
    secure testing tools are impractical or untrustworthy. In this paper, we discuss
    safety and reliability problems with the execution of LLM generated code and propose
    a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of
    executing LLM-generated code in a controlled and safe manner. STELP secures autonomous
    production AI systems involving code generation, filling the critical void left
    by the impracticality or limitations of traditional secure testing methodologies
    and human oversight. This includes applications such as headless code generation-execution
    and LLMs that produce executable code snippets as an action plan to be executed
    in real time. We contribute a human-validated dataset of insecure code snippets
    and benchmark our approach on publicly available datasets for correctness, safety,
    and latency. Our results demonstrate that our approach outperforms an existing
    method by a significant margin, particularly in its ability to safely execute
    risky code snippets. Warning: This paper contains malicious code snippets that
    should be run with caution.'
  tldr: '- Addresses the risk of deploying LLM-generated code directly into production
    systems, which can contain vulnerabilities like data poisoning, malicious attacks,
    and hallucinations leading to system malfunctions.

    - Proposes STELP, a secure transpiler and executor that runs LLM-generated code
    in a controlled environment, using safety checks and isolation to prevent harmful
    execution.

    - Matters for system security by enabling safe use of autonomous AI code generation
    in production, where human review and traditional testing are impractical or insufficient.'
  ai_score: 7
  ai_reason: The paper addresses safety and reliability for executing LLM-generated
    code, directly relating to memory safety, spatial safety, and sanitizer techniques,
    though its connection to type systems, formal semantics, and Rust is indirect.
  updated: '2026-01-13T17:55:11Z'
  published: '2026-01-09T01:49:41Z'
  pdf_url: https://arxiv.org/pdf/2601.05467v2
  primary_category: cs.SE
  categories:
  - cs.SE
  - cs.AI
