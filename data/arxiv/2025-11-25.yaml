title: Memory Safety / PL / Systems Radar
description: Recent arXiv papers filtered by my interests in systems security, PL,
  and memory safety.
generated_at: '2025-11-25T13:37:55.517007+00:00'
papers:
- arxiv_id: 2511.19130v1
  title: Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic
    Execution
  authors:
  - Rong Feng
  - Suman Saha
  summary: 'Obfuscation poses a persistent challenge for software engineering tasks
    such as program comprehension, maintenance, testing, and vulnerability detection.
    While compiler optimizations and third-party code often introduce transformations
    that obscure program intent, existing analysis tools and large language models
    (LLMs) struggle to recover the original semantics. In this work, we investigate
    whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively
    deobfuscate programs and restore analyzability. We construct a benchmark by applying
    four widely studied transformations-control-flow flattening, opaque predicates,
    arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation
    Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare
    three state-of-the-art LLMs under two training configurations: baseline fine-tuning
    on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE
    artifacts such as SMT constraints, path statistics, and test cases. Our evaluation
    examines syntactic correctness (compilation success), semantic fidelity (behavioral
    equivalence under symbolic execution), and code quality (readability and structure).
    Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and
    that incorporating KLEE artifacts consistently improves semantic preservation
    and compilation success across models. These findings highlight deobfuscation
    as a broader software engineering concern, demonstrating that combining LLMs with
    symbolic execution can strengthen automated testing, static analysis, and program
    comprehension in the presence of obfuscation.'
  tldr: '- Addresses the gap in automated tools'' ability to recover original program
    semantics from obfuscated code, which hinders vulnerability detection and program
    analysis.

    - Uses fine-tuned LLMs enhanced with symbolic execution artifacts (SMT constraints,
    path statistics, test cases) to deobfuscate programs transformed by control-flow
    flattening and encoding techniques.

    - Improves semantic preservation and compilation success in deobfuscated code,
    directly strengthening vulnerability analysis and security auditing capabilities
    for obscured software.'
  ai_score: 7
  ai_reason: This paper connects to program analysis and formal semantics through
    symbolic execution and semantic preservation evaluation, though it has limited
    direct focus on memory safety, type systems, or Rust-specific concerns.
  updated: '2025-11-24T13:55:20Z'
  published: '2025-11-24T13:55:20Z'
  pdf_url: https://arxiv.org/pdf/2511.19130v1
  primary_category: cs.SE
  categories:
  - cs.SE
- arxiv_id: 2511.18966v1
  title: 'LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large
    Language Models'
  authors:
  - Muhammad Usman Shahid
  - Chuadhry Mujeeb Ahmed
  - Rajiv Ranjan
  summary: The security of code generated by large language models (LLMs) is a significant
    concern, as studies indicate that such code often contains vulnerabilities and
    lacks essential defensive programming constructs. This work focuses on examining
    and evaluating the security of LLM-generated code, particularly in the context
    of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration
    (CWE) and, to study their criticality, mapped them to CVEs. We used ten different
    LLMs for code generation and analyzed the outputs through static analysis. The
    amount of CWEs present in AI-generated code is concerning. Our findings highlight
    the need for developers to be cautious when using LLM-generated code. This study
    provides valuable insights to advance automated code generation and encourage
    further research in this domain.
  tldr: '- Identifies that LLM-generated C/C++ code frequently contains known vulnerability
    types (CWEs) linked to real exploits (CVEs), creating security risks in production
    systems.

    - Uses static analysis to detect memory safety flaws (e.g., buffer overflows,
    use-after-free) in code produced by ten different LLMs across varied prompts.

    - Shows automated code generation introduces memory corruption risks unless rigorously
    validated, undermining system security foundations in unsafe languages.'
  ai_score: 7
  ai_reason: The paper evaluates security vulnerabilities in generated C/C++ code
    using static analysis and CWE categorization, directly relating to memory safety,
    spatial safety, and program analysis from the focus areas.
  updated: '2025-11-24T10:31:53Z'
  published: '2025-11-24T10:31:53Z'
  pdf_url: https://arxiv.org/pdf/2511.18966v1
  primary_category: cs.AI
  categories:
  - cs.AI
  - cs.CR
- arxiv_id: 2511.18782v1
  title: 'Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program
    repair?'
  authors:
  - Lukas Twist
  summary: Large Language Models (LLMs) often produce code with subtle implementation-level
    bugs despite strong benchmark performance. These errors are hard for LLMs to spot
    and can have large behavioural effects; yet when asked to summarise code, LLMs
    can frequently surface high-level intent and sometimes overlook this low-level
    noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline
    for program repair that leverages natural-language code summarisation as an explicit
    intermediate step, extending previous work that has already shown code summarisation
    to be a useful intermediary for downstream tasks. We evaluate our method across
    eight production-grade LLMs on two function level benchmarks (HumanEvalPack and
    MBPP), comparing several summary styles against a direct repair baseline. Error-aware
    diagnostic summaries consistently yield the largest gains - repairing up to 65%
    of unseen errors, on average of 5% more than the baseline - though overall improvements
    are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable
    diagnostic artefact that can be integrated into program-repair pipelines rather
    than a stand-alone fix-all.
  tldr: '- Addresses the problem that LLMs often generate code with subtle implementation
    bugs not caught during direct repair, despite strong high-level understanding
    shown in summaries.

    - Uses natural-language code summaries as an intermediate step in program repair,
    where error-aware diagnostic summaries help identify mismatches between intent
    and implementation.

    - Improves memory safety and low-level reliability by catching implementation
    flaws before deployment, reducing risks from latent bugs in generated code.'
  ai_score: 7
  ai_reason: The paper's focus on program repair through code summarization directly
    engages with program analysis and leverages aspects of type systems and formal
    semantics for identifying and fixing implementation-level bugs, aligning with
    the researcher's interests in program analysis and type systems.
  updated: '2025-11-24T05:33:38Z'
  published: '2025-11-24T05:33:38Z'
  pdf_url: https://arxiv.org/pdf/2511.18782v1
  primary_category: cs.SE
  categories:
  - cs.SE
- arxiv_id: 2506.06544v2
  title: Reasoning about External Calls
  authors:
  - Sophia Drossopoulou
  - Julian Mackay
  - Susan Eisenbach
  - James Noble
  summary: "In today's complex software, internal trusted code is tightly intertwined\
    \ with external untrusted code. To reason about internal code, programmers must\
    \ reason about the potential effects of calls to external code, even though that\
    \ code is not trusted and may not even be available. The effects of external calls\
    \ can be limited, if internal code is programmed defensively, limiting potential\
    \ effects by limiting access to the capabilities necessary to cause those effects.\n\
    \  This paper addresses the specification and verification of internal code that\
    \ relies on encapsulation and object capabilities to limit the effects of external\
    \ calls. We propose new assertions for access to capabilities, new specifications\
    \ for limiting effects, and a Hoare logic to verify that a module satisfies its\
    \ specification, even while making external calls. We illustrate the approach\
    \ though a running example with mechanised proofs, and prove soundness of the\
    \ Hoare logic."
  tldr: '- Addresses the challenge of reasoning about internal code safety when it
    interacts with untrusted external code, where external code effects are unknown
    and potentially harmful.


    - Introduces capability access assertions and effect-limiting specifications in
    a Hoare logic framework to verify that internal code maintains security properties
    despite external calls.


    - Enables formal verification that defensive programming using encapsulation and
    object capabilities effectively constrains external code''s potential damage to
    memory and system resources.'
  ai_score: 8
  ai_reason: The paper's focus on specification, verification, and Hoare logic for
    reasoning about external code effects directly addresses program analysis, formal
    semantics, and memory safety concerns central to the researcher's interests.
  updated: '2025-11-23T22:23:39Z'
  published: '2025-06-06T21:24:26Z'
  pdf_url: https://arxiv.org/pdf/2506.06544v2
  primary_category: cs.PL
  categories:
  - cs.PL
